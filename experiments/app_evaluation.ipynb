{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef4cfb9-f09a-48e3-8a54-c6216639d91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:01:14.944574Z",
     "iopub.status.busy": "2024-09-10T02:01:14.943969Z",
     "iopub.status.idle": "2024-09-10T02:01:14.956414Z",
     "shell.execute_reply": "2024-09-10T02:01:14.955865Z",
     "shell.execute_reply.started": "2024-09-10T02:01:14.944522Z"
    }
   },
   "source": [
    "# Evaluation of the NE application\n",
    "\n",
    "The goal of this notebook to evaluate the existing application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7002e453-4dfe-4696-bc4b-4cdd1b6a7001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T00:13:34.528764Z",
     "iopub.status.busy": "2024-11-08T00:13:34.518916Z",
     "iopub.status.idle": "2024-11-08T00:13:35.085142Z",
     "shell.execute_reply": "2024-11-08T00:13:35.084700Z",
     "shell.execute_reply.started": "2024-11-08T00:13:34.528534Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os \n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b9f8a2-b319-4958-a79b-196d558409c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T00:13:47.615426Z",
     "iopub.status.busy": "2024-11-08T00:13:47.614193Z",
     "iopub.status.idle": "2024-11-08T00:13:52.021395Z",
     "shell.execute_reply": "2024-11-08T00:13:52.021011Z",
     "shell.execute_reply.started": "2024-11-08T00:13:47.615327Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.10/site-packages/thinc/compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "/home/leo/.local/lib/python3.10/site-packages/thinc/compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n",
      "/home/leo/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-11-07 16:13:50.260819: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 16:13:50.287091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 16:13:50.833913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-07 16:13:51.347068: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-07 16:13:51.347088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: leo-Precision-5570\n",
      "2024-11-07 16:13:51.347091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: leo-Precision-5570\n",
      "2024-11-07 16:13:51.347132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.183.1\n",
      "2024-11-07 16:13:51.347140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.183.1\n",
      "2024-11-07 16:13:51.347142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.183.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's pre-trained English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf47c99-500e-4a03-a26c-0d7c5be2c1df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T00:13:54.933477Z",
     "iopub.status.busy": "2024-11-08T00:13:54.932347Z",
     "iopub.status.idle": "2024-11-08T00:13:54.943521Z",
     "shell.execute_reply": "2024-11-08T00:13:54.943171Z",
     "shell.execute_reply.started": "2024-11-08T00:13:54.933431Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_path, usecols=None, nrows=None):\n",
    "    df = pd.read_csv(file_path, usecols=usecols, nrows=nrows)\n",
    "    return df\n",
    "\n",
    "def save_results(output_file_path, metrics_file_path, df, metrics):\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Results saved into {output_file_path}, rows: {df.shape[0]}\")\n",
    "\n",
    "    print(metrics)\n",
    "    with open(metrics_file_path, \"w\", encoding='utf-8') as fp:\n",
    "        json.dump(metrics, fp, check_circular=True)\n",
    "    print(f\"Metrics saved into {metrics_file_path}\")\n",
    "    \n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_named_entities(text_list):\n",
    "    # Initialize result dictionary\n",
    "    result = {\n",
    "        'persons': [],\n",
    "        'organizations': [],\n",
    "        'locations': []\n",
    "    }\n",
    "\n",
    "    # Iterate through each text with SpaCy\n",
    "    for doc in nlp.pipe(text_list):\n",
    "        # Extract entities for each category\n",
    "        persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        organizations = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "        locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        \n",
    "        # Join entities with ';' or set as an empty string if no entities are found\n",
    "        result['persons'].append(\";\".join(persons) if persons else \"\")\n",
    "        result['organizations'].append(\";\".join(organizations) if organizations else \"\")\n",
    "        result['locations'].append(\";\".join(locations) if locations else \"\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_entities(df):\n",
    "    nes = extract_named_entities(df[\"text\"])\n",
    "    df.loc[:, \"persons_pred\"] = nes['persons']\n",
    "    df.loc[:, \"organizations_pred\"] = nes['organizations']\n",
    "    df.loc[:, \"locations_pred\"] = nes['locations']\n",
    "    return df\n",
    "\n",
    "def evaluate_binary(df):\n",
    "    def calculate_metrics(row):\n",
    "        true_set = set(row[column].split(';'))\n",
    "        pred_set = set(row[column+'_pred'].split(';'))\n",
    "        \n",
    "        tp = len(true_set & pred_set)  # True Positives\n",
    "        fp = len(pred_set - true_set)  # False Positives\n",
    "        fn = len(true_set - pred_set)  # False Negatives\n",
    "        support = len(true_set)        # Support: the number of true values (in 'persons')        \n",
    "        return pd.Series([tp, fp, fn, support])\n",
    "    \n",
    "    overall_metrics = {}    \n",
    "    for column in 'persons organizations locations'.split():\n",
    "        # Apply the function to each row\n",
    "        df[['TP_'+column, 'FP_'+column, 'FN_'+column, 'support_'+column]] = df.apply(calculate_metrics, axis=1)\n",
    "        \n",
    "        # Calculate precision, recall, F1 for each row\n",
    "        df['precision_'+column] = df['TP_'+column] / (df['TP_'+column] + df['FP_'+column])\n",
    "        df['recall_'+column] = df['TP_'+column] / (df['TP_'+column] + df['FN_'+column])\n",
    "        df['F1_'+column] = 2 * (df['precision_'+column] * df['recall_'+column]) / (df['precision_'+column] + df['recall_'+column])\n",
    "        \n",
    "        # Fill NaN values (where precision/recall is undefined) with 0\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Calculate overall precision, recall, F1-score, and support\n",
    "        overall_metrics[column] ={\n",
    "            \"precision\": round(sum(df['precision_'+column] * df['support_'+column]) /sum(df['support_'+column]), 3),\n",
    "            \"recall\": round(sum(df['recall_'+column] * df['support_'+column]) /sum(df['support_'+column]), 3),\n",
    "            \"F1\": round(sum(df['F1_'+column] * df['support_'+column]) /sum(df['support_'+column]), 3),\n",
    "            \"support\": int(df['support_'+column].sum())\n",
    "        }\n",
    "    return df, overall_metrics   \n",
    "\n",
    "def evaluate_baseline(extractor_name, nrows=None):\n",
    "    eval_file_path, eval_and_scores_file_path, metrics_file_path = get_files(extractor_name=extractor_name)\n",
    "\n",
    "    # Load data\n",
    "    used_columns = \"text persons organizations locations\".split()\n",
    "    df = load_data(eval_file_path, usecols=used_columns, nrows=nrows)\n",
    "    print(f\"Loaded {df.shape}\")\n",
    "\n",
    "    df.fillna('', inplace=True)\n",
    "    print(\"Extracting NE...\")\n",
    "    df_pred = predict_entities(df)\n",
    "    print(f'Extracted. Res df: {df.shape}, {df.columns}')\n",
    "    # print(df)\n",
    "\n",
    "    # Evaluate predictions against true labels\n",
    "    out_df, overall_metrics = evaluate_binary(df)\n",
    "\n",
    "    # Save the predictions to a CSV file (required format for submission)\n",
    "    save_results(eval_and_scores_file_path, metrics_file_path, df=out_df, metrics=overall_metrics)\n",
    "    print(\"Finish\")\n",
    "    return out_df, overall_metrics\n",
    "\n",
    "def get_files(extractor_name, data_dir=\"../data/external/hf\", eval_dataset='conll2003_transformed'):\n",
    "    eval_file_path = f\"{data_dir}/{eval_dataset}.csv\"\n",
    "    output_dir = f\"{data_dir}/{extractor_name}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created {output_dir} directory\")\n",
    "    file_prefix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    eval_and_scores_file_path = f\"{output_dir}/{file_prefix}.with_scores.csv\"\n",
    "    metrics_file_path = f\"{output_dir}/{file_prefix}.scores.json\"\n",
    "\n",
    "    return eval_file_path, eval_and_scores_file_path, metrics_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d176d69-56e7-46d5-8d1b-0420615fb19a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T00:14:21.607737Z",
     "iopub.status.busy": "2024-11-08T00:14:21.606796Z",
     "iopub.status.idle": "2024-11-08T00:14:21.760121Z",
     "shell.execute_reply": "2024-11-08T00:14:21.759439Z",
     "shell.execute_reply.started": "2024-11-08T00:14:21.607657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (100, 4)\n",
      "Extracting NE...\n",
      "Extracted. Res df: (100, 7), Index(['text', 'persons', 'locations', 'organizations', 'persons_pred',\n",
      "       'organizations_pred', 'locations_pred'],\n",
      "      dtype='object')\n",
      "Results saved into ../data/external/hf/spacy/20241107_161421.with_scores.csv, rows: 100\n",
      "{'persons': {'precision': 0.836, 'recall': 0.708, 'F1': 0.743, 'support': 161}, 'organizations': {'precision': 0.82, 'recall': 0.82, 'F1': 0.82, 'support': 100}, 'locations': {'precision': 0.933, 'recall': 0.867, 'F1': 0.89, 'support': 120}}\n",
      "Metrics saved into ../data/external/hf/spacy/20241107_161421.scores.json\n",
      "Finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'persons': {'precision': 0.836, 'recall': 0.708, 'F1': 0.743, 'support': 161},\n",
       " 'organizations': {'precision': 0.82,\n",
       "  'recall': 0.82,\n",
       "  'F1': 0.82,\n",
       "  'support': 100},\n",
       " 'locations': {'precision': 0.933,\n",
       "  'recall': 0.867,\n",
       "  'F1': 0.89,\n",
       "  'support': 120}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'external/hf'\n",
    "# eval_dataset = \"conll2003_transformed\"\n",
    "\n",
    "extractor_name = 'spacy'\n",
    "\n",
    "out_df, overall_metrics = evaluate_baseline(extractor_name=extractor_name, nrows=100)\n",
    "overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1340d-77d2-4b28-b207-14495cdb9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
