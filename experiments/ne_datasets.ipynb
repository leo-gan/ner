{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a0b1cb-81e1-4acf-af6d-cfdbbec0c8e6",
   "metadata": {},
   "source": [
    "# Named Entity datasets\n",
    "\n",
    "Here is code to download and prepprocess several well-known datasets across platforms like `Hugging Face` and `Kaggle`. These datasets are often used to train, fine-tune, and evaluate NER models, ranging from general-purpose to domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b764af4-f7fc-478b-b6f7-e8e38dc57c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:06:47.397170Z",
     "iopub.status.busy": "2024-10-31T17:06:47.396459Z",
     "iopub.status.idle": "2024-10-31T17:06:47.404958Z",
     "shell.execute_reply": "2024-10-31T17:06:47.404467Z",
     "shell.execute_reply.started": "2024-10-31T17:06:47.397110Z"
    }
   },
   "source": [
    "## Setting up\n",
    "\n",
    "Install all necessary packages and initialize the library names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643aeb0b-4812-4947-b4bf-a45bcd7d6661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:10:45.190125Z",
     "iopub.status.busy": "2024-10-31T17:10:45.189448Z",
     "iopub.status.idle": "2024-10-31T17:10:45.196238Z",
     "shell.execute_reply": "2024-10-31T17:10:45.195665Z",
     "shell.execute_reply.started": "2024-10-31T17:10:45.190072Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b043e05-a4fd-49f2-839a-128dbd4e4e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:10:54.614449Z",
     "iopub.status.busy": "2024-10-31T17:10:54.614009Z",
     "iopub.status.idle": "2024-10-31T17:10:54.618440Z",
     "shell.execute_reply": "2024-10-31T17:10:54.617948Z",
     "shell.execute_reply.started": "2024-10-31T17:10:54.614415Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers datasets pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5ab8b1-04a9-46af-933c-afd694e8ffd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:24:31.202040Z",
     "iopub.status.busy": "2024-10-31T17:24:31.200948Z",
     "iopub.status.idle": "2024-10-31T17:24:31.206557Z",
     "shell.execute_reply": "2024-10-31T17:24:31.205865Z",
     "shell.execute_reply.started": "2024-10-31T17:24:31.201958Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d8845-2c0f-4b3c-a21a-04599703c6eb",
   "metadata": {},
   "source": [
    "## Hugging Face datasets\n",
    "\n",
    "Find your dataset on the [Hugging Face Hub](https://huggingface.co/datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec007e-c4a9-4f13-b6bb-09009388ea5c",
   "metadata": {},
   "source": [
    "### CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b47c962-cc60-4dfb-854f-c84ef6dd2b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:13:37.593783Z",
     "iopub.status.busy": "2024-10-31T17:13:37.593431Z",
     "iopub.status.idle": "2024-10-31T17:13:41.087861Z",
     "shell.execute_reply": "2024-10-31T17:13:41.085311Z",
     "shell.execute_reply.started": "2024-10-31T17:13:37.593764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5a0901d0634ce68cf958490bc452cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c0013ec17f475e9be3152d8916b95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CoNLL-2003 dataset from Hugging Face\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b165dea-e7c7-44ed-a11e-1a3d5539408f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:42:06.558493Z",
     "iopub.status.busy": "2024-10-31T17:42:06.557884Z",
     "iopub.status.idle": "2024-10-31T17:42:07.308023Z",
     "shell.execute_reply": "2024-10-31T17:42:07.307168Z",
     "shell.execute_reply.started": "2024-10-31T17:42:06.558482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14041, 5) Index(['id', 'text', 'persons', 'locations', 'organizations'], dtype='object')\n",
      "(3250, 5) Index(['id', 'text', 'persons', 'locations', 'organizations'], dtype='object')\n",
      "(3453, 5) Index(['id', 'text', 'persons', 'locations', 'organizations'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the mapping from the dataset for the NER labels\n",
    "label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "def extract_entities(labels, tokens):\n",
    "    persons, locations, organizations = [], [], []\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    \n",
    "    for label, token in zip(labels, tokens):\n",
    "        # Convert integer labels to string tags\n",
    "        label_str = label_list[label]\n",
    "        \n",
    "        if label_str.startswith(\"B-\"):  # Beginning of a new entity\n",
    "            if current_entity:\n",
    "                # Save the previous entity\n",
    "                if current_type == \"PER\":\n",
    "                    persons.append(\" \".join(current_entity))\n",
    "                elif current_type == \"LOC\":\n",
    "                    locations.append(\" \".join(current_entity))\n",
    "                elif current_type == \"ORG\":\n",
    "                    organizations.append(\" \".join(current_entity))\n",
    "            # Start a new entity\n",
    "            current_entity = [token]\n",
    "            current_type = label_str[2:]\n",
    "        elif label_str.startswith(\"I-\") and current_type == label_str[2:]:\n",
    "            # Continuation of an entity\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            # No entity or different entity, save the current one\n",
    "            if current_entity:\n",
    "                if current_type == \"PER\":\n",
    "                    persons.append(\" \".join(current_entity))\n",
    "                elif current_type == \"LOC\":\n",
    "                    locations.append(\" \".join(current_entity))\n",
    "                elif current_type == \"ORG\":\n",
    "                    organizations.append(\" \".join(current_entity))\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "    \n",
    "    # Append the last entity if it exists\n",
    "    if current_entity:\n",
    "        if current_type == \"PER\":\n",
    "            persons.append(\" \".join(current_entity))\n",
    "        elif current_type == \"LOC\":\n",
    "            locations.append(\" \".join(current_entity))\n",
    "        elif current_type == \"ORG\":\n",
    "            organizations.append(\" \".join(current_entity))\n",
    "    \n",
    "    return \";\".join(persons), \";\".join(locations), \";\".join(organizations)\n",
    "\n",
    "# Processing the dataset\n",
    "def process_dataset(dataset_split):\n",
    "    rows = []\n",
    "    \n",
    "    for i, row in enumerate(dataset_split):\n",
    "        tokens = row['tokens']\n",
    "        labels = row['ner_tags']\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Extract entities from the labels and tokens\n",
    "        persons, locations, organizations = extract_entities(labels, tokens)\n",
    "        \n",
    "        rows.append({\n",
    "            'id': i,\n",
    "            'text': text,\n",
    "            'persons': persons,\n",
    "            'locations': locations,\n",
    "            'organizations': organizations\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Transform the train dataset\n",
    "# Use .select() if you want to limit the rows to process, or pass it directly for full dataset\n",
    "# train_df = process_dataset(dataset['test'].select(range(5)))  # For testing with the first 5 rows\n",
    "\n",
    "train_df = process_dataset(dataset['train']) \n",
    "validation_df = process_dataset(dataset['validation']) \n",
    "test_df = process_dataset(dataset['test'])\n",
    "\n",
    "_ = [print(df.shape, df.columns) for df in [train_df, validation_df, test_df]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89eafe83-7525-4941-8c54-5741e2f8b03d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T17:42:12.669862Z",
     "iopub.status.busy": "2024-10-31T17:42:12.669392Z",
     "iopub.status.idle": "2024-10-31T17:42:12.708067Z",
     "shell.execute_reply": "2024-10-31T17:42:12.707729Z",
     "shell.execute_reply.started": "2024-10-31T17:42:12.669828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (14041, 5) ../data/external/hf/conll2003_transformed.train.csv\n",
      "Saved (3250, 5) ../data/external/hf/conll2003_transformed.validation.csv\n",
      "Saved (3453, 5) ../data/external/hf/conll2003_transformed.test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "def save_df(df, name):\n",
    "    file_name = f\"../data/external/hf/conll2003_transformed.{name}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Saved {df.shape} {file_name}\")\n",
    "\n",
    "_ = [save_df(df, name) for df, name \n",
    "     in [(train_df, \"train\"),(validation_df, \"validation\"),(test_df, \"test\"),]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79986973-ebf0-4cbc-98f7-9494db7963ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T04:24:38.309136Z",
     "iopub.status.busy": "2024-11-01T04:24:38.307455Z",
     "iopub.status.idle": "2024-11-01T04:24:38.347438Z",
     "shell.execute_reply": "2024-11-01T04:24:38.346915Z",
     "shell.execute_reply.started": "2024-11-01T04:24:38.309072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (20744, 5) ../data/external/hf/conll2003_transformed.all.csv\n"
     ]
    }
   ],
   "source": [
    "# Save compounded df, since we use the pretrained model and use df only for evaluation:\n",
    "\n",
    "all_df = pd.concat([train_df, validation_df, test_df])\n",
    "assert all_df.shape[0] == (train_df.shape[0] + validation_df.shape[0] + test_df.shape[0])\n",
    "save_df(all_df, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9d595-9f1e-4cf9-b7c3-6201535bdc5a",
   "metadata": {},
   "source": [
    "## Label my personal dataset TODO\n",
    "\n",
    "TODO \n",
    "It should be moved into another NB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc5c8a-ba22-417c-9fc9-f0c064607822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path because we use code from the application\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    sys.path.append(parent_dir+'/ne_extractor_app')\n",
    "    print(f\"added {parent_dir}\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572eb2a-5336-4536-add4-baf8d5d91223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ne_extractor_app.app.ne_extractor import NEExtractor\n",
    "\n",
    "import typer\n",
    "from ne_extractor_app.app.evaluation import evaluate\n",
    "from ne_extractor_app.app.models.ensemble import EnsembleNERModel\n",
    "from ne_extractor_app.app.name_normalizer import Normalizer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "model = EnsembleNERModel()\n",
    "normalizer = Normalizer()\n",
    "extractor = NEExtractor(ne_model=model, normalizer=normalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84313b20-c9a8-4262-bbd6-83f632917acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
